# Optimization-Algorithms-for-Non-Convex-Problems

This is part of project requirement for course IOE 511 (Continuous Optimization Methods) at University of Michigan, Ann Arbor. 

We were require to implement following algorithm to solve 10 set of problems (refer to Project_Problems. pdf). These problems were quadratic and non linear. The algorithms implemented were: 
a. Gradient Descent - (Wolfe Line Search, Backtracking Line Search)
b. Modified Newton Raphson Method - (Wolfe Line Search, Backtracking Line Search))
c. Trust region Newton with Conjugate Gradient subproblem solver
d. Symmetric Rank 1 Method with Quasi-Newton with Conjugate Gradient subproblem solver
e. BFGS quasi-Newton - (Wolfe Line Search, Backtracking Line Search)
g. DFP quasi-Newton - (Wolfe Line Search, Backtracking Line Search)
